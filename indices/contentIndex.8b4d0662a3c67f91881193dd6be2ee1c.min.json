{"/":{"title":"Kohaku","content":"## About\n我是 Kohaku，一名软件开发工程师，这是我的个人数字花园，用来记录平时的闪念笔记以及总结内容。\n\n## Content\n### 大数据学习\n[[大数据/转行数据理由]]\n\n[[大数据/实时数据 Flink 路线图]]\n\n[[大数据/Flink 基本原理]]\n\n### 兴趣爱好\n[[漫画]]\n\n[[动画/动画]]\n\n### 年度计划\n[[年度计划]]\n\n### 其他\n[[收藏句子]]\n","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%8A%A8%E7%94%BB/%E5%8A%A8%E7%94%BB":{"title":"动画","content":"待补充……","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98":{"title":"分布式缓存","content":"\n分布式缓存（Distributed Cache）是一种在分布式计算环境中存储和共享数据的技术。它允许在计算集群的各个节点之间共享数据，以便更快、更高效地进行计算。以下几个关键点可以帮助理解分布式缓存的基本概念：\n\n1. 数据共享：分布式缓存允许在分布式计算节点之间共享数据。这可以确保各计算节点可以访问到相同的数据，从而提高计算性能和一致性。\n\n2. 缓存：分布式缓存通常将数据存储在内存中，以便在计算过程中快速访问。这有助于减少数据访问延迟，提高计算效率。\n\n3. 一致性：分布式缓存需要在不同节点之间维护数据的一致性。这通常需要采用一些同步和数据复制技术，以确保各节点上的数据保持一致。\n\n在 Flink 中，分布式缓存处于辅助性地位，主要用于提高数据访问性能和简化数据共享。Flink 提供了一种简单的分布式缓存机制，允许用户将本地或远程文件作为缓存文件注册到 Flink 任务中。这些缓存文件会被分发到所有计算节点，并在本地文件系统中缓存。然后，在 Flink 任务的处理过程中，用户可以通过分布式缓存 API 快速访问这些缓存文件。\n\n在 Flink 中使用分布式缓存的典型场景包括：\n\n1. 外部数据访问：当 Flink 任务需要访问外部数据（如配置文件、辅助数据等）时，可以使用分布式缓存将这些数据分发到各计算节点，以便在处理过程中快速访问。\n\n2. 数据共享：在进行数据处理时，如果需要在计算节点之间共享部分数据，可以使用分布式缓存将这些数据存储和共享。\n\n总之，分布式缓存在 Flink 中主要用于提高数据访问性能和简化数据共享。通过数据共享、缓存和一致性等特性，分布式缓存可以在分布式计算环境中提高计算性能和一致性。虽然分布式缓存不是 Flink 核心原理的一部分，但它在实际应用中发挥了重要的辅助作用。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E8%AF%AD%E8%A8%80%E9%80%89%E5%9E%8B":{"title":"实时数据开发语言选型","content":"\n\u003e 以下回答均由 GPT4 生成，后续会根据实际使用情况做更改\n\n### 问：做 Flink 实时流数据开发使用 Python 还是 Scala 更好？\n在实时流数据处理中，Apache Flink 提供了对 Java、Scala 和 Python 的支持。然而，在性能和生态系统方面，Scala 和 Java 通常比 Python 更优越。以下是关于在 Flink 实时流数据开发中选择 Python 和 Scala 的一些比较：\n\n1. 性能：\n   - Scala：作为一种运行在 Java 虚拟机（JVM）上的静态类型语言，Scala 具有很好的性能。Scala 与 Java 共享底层运行时，因此在性能方面与 Java 类似。\n   - Python：Python 是一种动态类型语言，通常在性能方面不如 Scala。Flink 使用 PyFlink 支持 Python API，但由于全局解释器锁（GIL）和其他原因，Python 在性能上通常不如 JVM 语言（如 Scala 和 Java）。\n\n2. 生态系统和社区支持：\n   - Scala：Flink 本身是用 Scala 编写的，因此在 API 和功能方面与 Scala 有很好的兼容性。Scala 在大数据和流处理领域有丰富的生态系统和社区支持。\n   - Python：虽然 Flink 支持 Python API，但与 Scala 和 Java 生态系统相比，Python 社区在 Flink 方面的支持相对较弱。\n\n3. 编程模型和语法：\n   - Scala：Scala 是一种功能强大且表达性丰富的编程语言，具有优雅的语法和强大的类型系统。Scala 允许你使用函数式编程和面向对象编程范式，易于编写简洁、可读的代码。\n   - Python：Python 是一种非常流行且易于学习的编程语言，语法简单直观。这使得使用 Python 编写代码变得更加容易，尤其是对于初学者。\n\n综上所述，对于 Flink 实时流数据开发，建议选择 Scala，因为它在性能和生态系统方面具有优势。然而，如果你的团队已经熟悉 Python 且对性能要求不高，可以考虑使用 Python 进行开发。在实际项目中，选择合适的语言取决于团队的技能、项目需求和性能要求。\n\n### 问：Java 和 Scala 相比呢，哪种更适合开发 Flink 实时流数据\n在选择 Java 和 Scala 作为 Flink 实时流数据处理开发语言时，二者各有优势。以下是 Java 和 Scala 的一些比较：\n1. 性能：\n   - Java：Java 是一种运行在 Java 虚拟机（JVM）上的静态类型语言，具有良好的性能。\n   - Scala：Scala 同样运行在 JVM 上，与 Java 共享底层运行时。因此，在性能方面，Java 和 Scala 都具有相似的优势。\n\n2. 生态系统和社区支持：\n   - Java：Java 拥有庞大的生态系统和社区支持，是实时流处理和大数据领域的主要编程语言之一。\n   - Scala：Flink 本身是用 Scala 编写的，因此在 API 和功能方面与 Scala 有很好的兼容性。Scala 同样在大数据和流处理领域拥有丰富的生态系统和社区支持。\n\n3. 编程模型和语法：\n   - Java：Java 是一种面向对象的编程语言，语法相对简单。Java 在 Java 8 及更高版本中引入了对 Lambda 表达式和 Stream API 的支持，使得编写函数式编程风格的代码变得更加简单。\n   - Scala：Scala 是一种功能强大且表达性丰富的编程语言，支持函数式编程和面向对象编程范式。Scala 的语法更加简洁和优雅，使得编写可读性更强的代码变得更加容易。\n\n综上所述，从性能和生态系统的角度看，Java 和 Scala 都适合用于 Flink 实时流数据处理开发。在实际项目中，选择哪种语言取决于团队的技能和编程风格偏好。如果团队熟悉 Java 且更倾向于面向对象编程，那么 Java 可能是一个更好的选择。而如果团队熟悉 Scala 且喜欢函数式编程风格，Scala 可能是更适合的选择。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE-Flink-%E8%B7%AF%E7%BA%BF%E5%9B%BE":{"title":"实时数据 Flink 路线图","content":"\n\u003e 以下是 GPT4 生成的为期三个月的 Flink 实时数据开发的路线图，每个技能及学习的知识都列出了具体时限以及实践内容\n\n了解你的需求。下面是一个针对 Flink 实时数据开发（使用 Java）的学习路线图，其中每个阶段都包含实践内容：\n\n1. Java 基础加强（1 周）：\n   - 学习 Java 面向对象编程（OOP）的概念，如类、对象、继承、封装和多态。\n   - 熟悉 Java 集合框架，如 List、Set、Map 等。\n   - 学习 Java 8 新特性，如 Lambda 表达式、Stream API 和 Optional 类。\n   - 实践：完成一些简单的 Java 编程练习，如实现数据结构和算法。\n\n2. 数据库与 SQL（1 周）：\n   - 学习关系型数据库的基本概念，如表、行、列、主键、外键等。\n   - 学习 SQL 语言，包括 DDL（数据定义语言）、DML（数据操作语言）和查询操作。\n   - 熟悉至少一种关系型数据库管理系统（如 MySQL、PostgreSQL 或 Oracle）的使用。\n   - 实践：创建一个简单的数据库并编写 SQL 查询来实现数据操作。\n\n3. Apache Kafka 简介（3 天）：\n   - 学习 Apache Kafka 的基本概念，如生产者、消费者、主题和分区。\n   - 了解如何使用 Java 编写 Kafka 生产者和消费者。\n   - 实践：搭建一个简单的 Kafka 集群，并使用 Java 编写一个生产者和消费者程序。\n\n4. Apache Flink 入门（2 周）：\n   - 学习 Flink 的核心概念，如流处理、有界和无界数据集、窗口操作和时间处理。\n   - 学习 Flink 的 DataStream API 和 ProcessFunction。\n   - 学习如何使用 Java 进行 Flink 编程。\n   - 实践：使用 Flink 处理 Kafka 中的实时数据，并实现一些简单的数据转换和窗口操作。\n\n5. Flink 中的状态管理和容错（1 周）：\n   - 学习 Flink 中的状态管理和容错机制，如状态后端、检查点和恢复。\n   - 学习如何使用 Flink 的状态 API 进行状态管理。\n   - 实践：在 Flink 程序中使用状态管理，并设置检查点以确保容错。\n\n6. Flink 应用优化与调试（1 周）：\n   - 学习如何监控和调试 Flink 应用。\n   - 了解 Flink 应用的性能调优策略，如资源分配、并行度和缓存。\n   - 实践：监控和调试一个 Flink 实时数据处理应用，并尝试进行性能优化。\n\n7. 实战项目（1 周）：\n   - 根据所学知识，选择一个实际的实时数据处理场景，设计一个 Flink 数据处理项目。\n   - 使用 Java 编写 Flink 程序，处理来自 Kafka 的实时数据流，实现数据清洗、转换、窗口聚合等操作。\n   - 对 Flink 程序进行优化和调试，确保稳定性和性能。\n   - 将处理结果存储到关系型数据库或其他存储系统中。\n\n在学习过程中，请确保将所学知识应用于实际场景，并积累实践经验。参考优质的在线教程、书籍和官方文档，以便更好地学习和巩固知识。此外，可以积极参与 Flink 社区，提问和解答问题，以便更好地学习和巩固知识。\n\n完成这个学习路线图后，你应该能够入门 Flink 实时数据开发并具备完成基础开发任务的能力。不过，请记住，学习是一个持续的过程，你需要不断地实践、探索和更新知识。祝你学习顺利！","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F":{"title":"广播变量","content":"\n广播变量（Broadcast Variables）是一种特殊的数据结构，用于在分布式计算环境中共享数据。在许多大数据处理任务中，广播变量可以有效解决数据共享和同步的问题。以下几个关键点可以帮助理解广播变量的基本概念：\n\n1. 全局共享：广播变量是一个全局共享的数据结构，它可以被所有计算节点访问。这意味着广播变量中的数据可以在整个计算集群中传播和共享，从而确保所有节点可以访问到相同的数据。\n\n2. 只读：广播变量是只读的，这意味着一旦创建并广播，它的内容就不能再修改。这有助于保持数据的一致性，并避免在分布式环境中出现数据竞争和同步问题。\n\n3. 缓存：广播变量通常会在每个计算节点上进行缓存，以便在处理过程中快速访问。这有助于减少数据传输和通信开销，提高计算性能。\n\n在 Flink 中，广播变量处于辅助性地位，主要用于解决数据共享和同步的问题。Flink 支持在 DataStream API 和 DataSet API 中使用广播变量。在 Flink 中使用广播变量的典型场景包括：\n\n1. 参数共享：当需要在分布式计算过程中共享参数或配置信息时，可以使用广播变量。例如，在机器学习任务中，可以将模型参数广播到所有计算节点，以便在进行预测或更新时保持一致性。\n\n2. 小表连接：在进行数据连接操作时，如果其中一个数据集较小，可以将其作为广播变量，以便在所有计算节点上进行高效连接。这有助于减少数据传输和通信开销，提高连接性能。\n\n总之，广播变量在 Flink 中主要用于解决数据共享和同步的问题。通过全局共享、只读和缓存等特性，广播变量可以在分布式计算环境中保持数据的一致性，并提高计算性能。虽然广播变量不是 Flink 核心原理的一部分，但它在实际应用中发挥了重要的辅助作用。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4":{"title":"数据处理时间","content":"\n在 Flink 中，数据处理时间是一个重要概念，它与数据处理过程中的时间相关性有关。Flink 支持两种主要的时间概念：事件时间（Event Time）和处理时间（Processing Time）。这两种时间概念有各自的特点和用途，我们将分别讨论它们。\n\n1. 事件时间（Event Time）：\n事件时间是指数据元素产生时的时间戳。这意味着数据中的每个元素都有一个与之相关的事件时间。这个时间通常是由数据产生源（如传感器、日志记录器等）记录的。Flink 使用水位线（Watermark）机制来处理事件时间。水位线是一种特殊的数据元素，表示某一时刻之前的所有事件已经到达系统。这可以帮助 Flink 确定何时可以对基于事件时间的窗口进行计算和输出。\n\n使用事件时间的优势在于可以保证处理的结果与数据发生的时间顺序一致，即使数据源产生了乱序或延迟数据。这对于需要准确计算基于事件发生时间的统计信息、分析和报告的应用场景非常重要，例如金融交易分析、传感器数据处理等。\n\n2. 处理时间（Processing Time）：\n处理时间是指 Flink 接收到数据元素时的系统时间。处理时间依赖于数据元素到达 Flink 系统的顺序和速度。由于数据到达的顺序可能与实际发生的顺序不同，因此基于处理时间的计算可能会受到数据延迟和乱序的影响。\n\n处理时间的优势在于它简化了时间处理逻辑，因为 Flink 只需考虑当前系统时间。此外，处理时间通常具有较低的延迟，因为它不需要等待水位线或其他时间同步机制。这对于需要快速响应和实时反馈的应用场景非常有用，例如实时监控和报警。\n\n在实际应用中，选择事件时间还是处理时间取决于对结果准确性和处理延迟的要求。如果需要考虑数据发生的时间顺序以获取准确的统计结果，那么事件时间是更好的选择；如果对实时性有较高要求，可以选择处理时间。Flink 也支持同时使用事件时间和处理时间，以便在不同的计算阶段或任务中根据需求选择合适的时间概念。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%8A%B6%E6%80%81%E5%92%8C%E5%AE%B9%E9%94%99":{"title":"状态和容错","content":"\n在 Flink 中，状态（State）和容错（Fault Tolerance）是两个关键概念，它们确保了数据处理过程的可靠性和一致性。我们将分别讨论这两个概念：\n\n1. 状态（State）：\n状态是 Flink 任务执行过程中的中间数据。在处理数据流时，任务可能需要根据之前处理过的数据元素来决定如何处理当前的数据元素。这些之前处理过的数据以状态的形式存储。状态可以是简单的计数器、列表，也可以是复杂的数据结构，如哈希表、窗口缓冲区等。\n\nFlink 提供了丰富的状态类型和状态访问接口，以便用户可以根据需要选择合适的状态类型，并在任务中方便地操作状态。Flink 还支持状态后端（State Backend）的概念，用于存储和管理状态数据。状态后端可以是内存、文件系统或分布式存储系统，具体取决于性能、持久性和可扩展性等需求。\n\n2. 容错（Fault Tolerance）：\n容错是指在面对系统故障（如节点宕机、网络故障等）时，Flink 能够保证数据处理的正确性和一致性。为了实现容错，Flink 采用了一种称为“检查点”（Checkpoint）的机制。检查点是 Flink 任务执行过程中的某个时刻的状态快照。Flink 会周期性地将任务的状态保存到检查点中，并将检查点存储到可靠的外部存储系统（如分布式文件系统）。\n\n当发生故障时，Flink 可以从最近的检查点恢复任务执行。这意味着 Flink 会重新加载检查点中的状态数据，并从检查点对应的数据流位置开始重新处理数据。这样，Flink 可以保证在故障发生后，数据处理可以从一个已知的正确状态开始，并保证处理结果的一致性。\n\n为了减小故障恢复时的数据丢失，Flink 还支持端到端的一致性保证，如“精确一次”（Exactly-Once）处理语义。精确一次处理语义确保每个数据元素在 Flink 任务中仅被处理一次，即使在发生故障时。这需要 Flink 与数据源和数据接收方协同工作，以确保数据的正确传输和处理。\n\n总之，状态和容错是 Flink 中两个关键概念，它们确保了数据处理过程的可靠性和一致性。通过使用状态来存储和管理任务的中间数据，以及使用检查点和一致性保证来实现容错，Flink 可以在面对系统故障时","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E8%BD%AC%E8%A1%8C%E6%95%B0%E6%8D%AE%E7%90%86%E7%94%B1":{"title":"转行数据理由","content":"\n就目前就业市场来看前后端这类开发岗位基本上叫苦连天，很多人找不到工作(这是比较真实的情况)，而大数据开发类型的我在网上和有关求职 APP 上看到的抱怨较少而且开的工资普遍比开发岗位要来的多。仔细了解后更是觉得大数据这行会在 AI 蓬勃发展后跟着发展起来，未来会有越来越多的人去了解大数据去学习大数据，毕竟 AI 是绝对离不了数据的，好的 AI 更是如此。\n\n(未完待续……)","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97":{"title":"迭代计算","content":"\n迭代计算是一种反复执行计算过程，直到满足某种终止条件的计算方法。在许多数据处理任务中，迭代计算是一种常见的处理模式，尤其是在图计算、机器学习等领域。迭代计算的基本概念可以通过以下几个关键点来理解：\n\n1. 初始状态：迭代计算开始时，需要一个初始状态。这个状态通常包含一些基本的数据和参数，用于启动迭代过程。\n\n2. 迭代函数：迭代函数是一个用于更新状态的函数。在每次迭代中，迭代函数会根据当前状态计算新的状态。迭代函数可以是简单的数学公式，也可以是复杂的数据处理逻辑。\n\n3. 终止条件：终止条件是一个用于判断迭代是否结束的条件。当终止条件满足时，迭代计算停止，输出最终结果。终止条件可以是迭代次数、误差阈值等。\n\n在 Flink 中，迭代计算是一个重要的处理模式。Flink 支持在数据流中执行迭代计算，这使得 Flink 可以处理那些需要反复执行计算过程的任务，例如图计算、机器学习等。Flink 提供了两种迭代计算模式：批量迭代和流式迭代。\n\n1. 批量迭代（DataSet API）：\nFlink 的 DataSet API 支持批量迭代计算。在这种模式下，迭代计算是基于批处理数据集进行的。批量迭代在每次迭代中处理整个数据集，然后更新状态。当满足终止条件时，批量迭代结束，并输出最终结果。批量迭代适用于那些需要对整个数据集进行全局分析的任务，如图计算、聚类分析等。\n\n2. 流式迭代（DataStream API）：\nFlink 的 DataStream API 支持流式迭代计算。在这种模式下，迭代计算是基于实时数据流进行的。流式迭代可以在数据流中连续处理数据，实现低延迟的迭代计算。流式迭代适用于那些需要实时反馈和动态调整的任务，如在线学习、实时推荐等。\n\n总之，迭代计算在 Flink 中处于重要地位，它使得 Flink 可以处理那些需要反复执行计算过程的任务。通过批量迭代和流式迭代两种模式，Flink 能够满足不同类型的迭代计算需求，提供灵活的数据处理能力。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86":{"title":"Flink 基本原理","content":"\nFlink 是一个分布式大数据处理框架，它可以处理大量数据，并提供快速、可靠和实时的数据处理能力。Flink 的基本原理可以通过以下几个简单的概念来理解：\n\n1. 数据流：Flink 使用数据流的概念来表示处理的数据。数据流是一个连续的数据元素序列，这些元素被处理、转换和输出到最终的结果中。简单说，数据流就是从数据源（如文件、数据库等）读取的数据，在 Flink 中经过一系列处理操作，最后输出到目的地（如另一个文件、数据库等）。\n\n2. 节点和任务：**Flink 任务是数据处理的基本单元。一个任务可以是一个简单的数据转换，例如过滤、映射等，也可以是更复杂的操作，例如聚合、排序等**。任务在 Flink 中以节点的形式存在。**节点是构成 Flink 数据流图的基本元素，数据流图是 Flink 对数据处理流程的一种抽象表示**。\n\n3. 并行处理：Flink 具有很好的并行处理能力。这意味着它可以将一个任务划分为多个子任务，在多个计算资源（如 CPU、内存等）上同时处理，以提高整体处理速度。Flink 会自动将任务划分为多个子任务，并行执行，最后将结果汇总。\n\n4. [[大数据/数据处理时间]]：Flink 支持不同的时间概念，包括事件时间（Event Time）和处理时间（Processing Time）。事件时间是数据元素产生的时间，处理时间是 Flink 接收到数据元素的时间。这两种时间概念可用于处理数据时的时间窗口计算，根据实际需求选择合适的时间概念。\n\n5. [[大数据/状态和容错]]：Flink 具有状态管理功能，可以在任务执行过程中存储和管理数据的状态。为了确保数据处理的可靠性，Flink 采用了一种称为“检查点”（Checkpoint）的机制。在执行过程中，Flink 会周期性地将任务的状态保存到外部存储系统，以便在发生故障时从最近的检查点恢复任务执行。\n\n简单来说，Flink 是一个强大的大数据处理框架，它可以处理大量数据，并提供实时、可靠和高效的数据处理能力。通过数据流、节点和任务、并行处理、时间概念和状态管理等核心概念，Flink 能够满足不同类型的数据处理需求。\n\n[[大数据/迭代计算]]\n\n[[大数据/广播变量]]\n\n[[大数据/分布式缓存]]\n","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E5%B9%B4%E5%BA%A6%E8%AE%A1%E5%88%92":{"title":"年度计划","content":"\n转眼间2023年也已过半，但是自身感觉却没过多长时间，这可能是因为把时间都放到了一成不变的事情上去并没有取得多大的进步和成就。在之前的 obsidian 仓库中也列出了年度计划，现在要在之前的计划上有更多的增补。\n\n首先就是工作与副业的取舍，这两个于我而言不太可能都能取得较大成功因为我是一个只能把心思花在一个地方的人而且没有天才的头脑。相比于工作，在业余时间找自己喜欢的东西并且坚持下去也绝对不是一件容易的事情，两者最大的区别在于找自己喜欢或者擅长的事情去做不会有那么多质疑和苦恼，难处是坚持下去。对于我这种人而言很难在工作中找到特别大的成就感特别是目前公司是保密无法联网类型的，很多问题和经验无法及时总结到我的知识库中，给我一种很强烈的割裂感：工作就是工作，无法给自己带来个人经验上的成长。而且我也很难忍受一天要一直加班到8点半回到家就是9点的生活，我希望的工作是6点前下班晚上能有更多的时间留给自己，所以我决定选择副业或者说把时间花在自己喜欢和擅长的事情上去。\n\n然后就是爱吃和减脂的取舍，减脂路上最困难的事情就是管住自己的嘴不要乱吃东西，","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%94%B6%E8%97%8F%E5%8F%A5%E5%AD%90":{"title":"收藏句子","content":"\n\u003e 当你停止创造，你的才能就不再重要，你所拥有的只剩下你的品味。而品味会裹挟你，让你排斥他人，变得狭隘。所以，要创造。\n\n\u003e Ars longa, vita brevis\n\u003e 艺术长久，生命短暂\n\n","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%95%99%E7%A8%8B/obsidian%E6%9C%AC%E5%9C%B0%E5%86%85%E5%AE%B9%E5%90%8C%E6%AD%A5%E5%88%B0%E6%95%B0%E5%AD%97%E8%8A%B1%E5%9B%AD":{"title":"obsidian本地内容同步到数字花园","content":"\n## 准备工作\n1. 首先 fork 两个模板项目作为参考","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-09":{"title":"2023-04-09","content":"\n每天花在电脑上的时间实在是太长了，准确来说是电脑消遣方面。上电脑无非就是几件事情：看视频消遣、玩游戏消遣、看 TradingView 和 Twitter 消磨时光，除此以外几乎不会拿电脑做任何正事或者是有意义的事情。不论是平时还是周末，几乎都只是在电脑前坐着然后做上述几件事情来消磨时光，这在我看来很可悲，因为这样做的意义也只是让思维僵化然后循规蹈矩地做这几件已经习以为常的事情。\n\n1. 工作时间内专心一些，最好把时间都放入到写代码完成业务中而不是抱着完成任务的心态去做工作\n2. 减少面对电脑或者手机的时间增加看纸质书的时间，过去几个月几乎没有翻过书，没有看书的日子就是没头苍蝇很迷茫的日子，只能靠电脑来消磨时光\n3. 在自己的专业领域投入更多的时间，不仅仅是技术上需要精进更重要的是理解自己为何而工作，什么样的工作能让自己持之以恒地做下去不会一段时间一个想法","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-10":{"title":"2023-04-10","content":"学习网页上传文件并处理功能[[网页上传原理]]，对于上传的整体流程有了更为深入的理解","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-11":{"title":"2023-04-11","content":"\n### 工作心态和个人心态的平衡\n工作中很难避免地会出现很多指标，当指标变得越来越多的时候人的心态就会出现变化特别是指标变成强制而且和绩效挂钩的时候，能在工作中全身心投入到代码开发或者学习提升的时间变得越来越少，而时间基本都放到如何将指标提升上去这一行为中。\n\n而实际上指标完成率高和是否把事情做好基本没有太大的关系，很多时候员工只会想方设法地做一些毫无意义的事情或者瞒报虚报来把指标提高，特别是领导特别重视或者说本身就是靠着这种方法上位的人则更是这样。久而久之就会发现只要有问题就会想法设法地拉会议提出解决方法然后要求你闭环，但是会议的主题却并不是当机立断地结合实际问题把解决问题的方法确定下来而是想法设法出文档出方案要求用数据或者指标闭环来确保这个事情确实得到了解决，最终的结果大多就是 wiki 文档或者 wps 云文档上又多出了一份垃圾来增加搜索真正有用文档的难度。\n\n那么如何平衡自己工作时的心态以及平时生活的心态来让自己不要把怒气代入到工作中呢？我目前的心得体验是工作的时候尽量认真对待，但是需要每天从工作时间抽出固定的时间留给自己去做总结和提炼(例如今天写业务时碰到的问题是如何解决的，通过这个问题我学到了什么)，遇到指标上的事情照做不误就是了因为是公司要求的而又是它给我钱，反正我工作时间就这么多，其他事情做不完直接和领导说就是不要自己烦扰自己。最主要的就是每天要有所收获，无论是工作上还是工作外，这样才有持续生活的动力才不会觉得自己的生活就只有工作。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-12":{"title":"2023-04-12","content":"\n今天在工作中没有做总结，工作中主要做了两件事：帮其他组重建 React 项目，搭了包括配置项、请求封装、git 钩子等等脚手架，整体基于 umimax。跟进测试问题，修复了负数数字没有动画的问题。\n\n使用 umimax 最新版的时候遇到了两个之前没碰到过的问题：\n1. VS Code 自带 [[TypeScript]] 编译器可能和项目安装的 TypeScript 版本不一致，如果项目内版本更高一些则会导致 tsconfig.json 配置文件报错从而导致整个项目的导入都有问题，有两个解决方案，一个是降级项目内的 TypeScript 版本另外一个就是设置 VS Code 的 TypeScript 版本，一般推荐第二种。第二种设置之后会在项目顶端生成 .vscode 文件夹，里面放着 setting.json 配置文件指定了 VS Code 配置项。\n2. 作业条请求封装的问题，之前从来没接触过作业条后端所以今天相当于重新学习了这种请求体如何封装以及返回的参数是什么，由于项目内基本都是作业条请求所以无需做过剩的封装，做一层简单的错误处理以及拿到响应体里面需要的数据即可。\n\n做事要认真这个没问题，但是同时也要结合实际业务场景去考虑一些事情是否需要做的如此复杂例如考虑的请求封装，由于项目内全是作业条请求所以基本不需要做过多的封装，完成后时间可以投入到其他事情中去。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-14":{"title":"2023-04-14","content":"\n","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-16":{"title":"2023-04-16","content":"\n最近的生活状态实在不太规律，经常性的1-2点睡觉然后7点半左右起床，平均下来每天的睡眠时间可能只有6个小时不到。而且由于中午55才下班，回去做完饭洗完碗可能都会一点过不少了，这个只能自己适应：要不就午睡时间缩短一点要不就提前晚上做好第二天的饭然后加热就行。\n\n最大的问题是我平均睡眠时间短是因为玩游戏的时间太多了，或者说固态化三件套时间太多了：基本就是 Youtube、Twitter、Tradingview 三个软件之间来回切换，然后除此以外的时间都在玩 Apex，基本没有时间放在认真读书和研究技能上的事情。减脂这件事情也因为不规律的生活导致进度拖延：这两周体重基本没有变化，虽然有持续在运动(因为吃的东西方面没有控制，经常中午不想做饭了就去吃麦当劳)。\n\n最近的表现实在是太不尽如人意，随着年龄的越来越大逐渐发现有些事情无法改变，但是有一点是我可以肯定的：只要持之以恒地做某件事情，那么这件事情在手中就会做的越来越好，起初可能完全带来不了任何收益而且会占用大量的时间但是随着时间的推进能做到的事情要远超自己想象。**我会先从放弃网游和看没营养的视频开始把时间慢慢地挪出来做更有意义的事情。**\n\n明天要做的最重要的事情就是把 [[年度计划]] 都大致列一个，看看哪些事情对目前的自己是最重要的，如何去达成它们，不在乎最终达成的结果而是在意过程如何。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-17":{"title":"2023-04-17","content":"\n最近工作上需要使用代码开发重构之前使用可视化平台开发的作业，[[大数据/实时数据开发语言选型]]上斟酌了很久，最后决定采用 Java 开发。组内没有任何使用代码开发数据的经验，前期选型语言上就花费了大量的时间，需要慢慢结合实践去熟悉 Java 的使用以及流数据代码开发。为此也根据 GPT4 列出实时数据代码开发的[[大数据/实时数据 Flink 路线图|路线图]]。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-18":{"title":"2023-04-18","content":"\n[[大数据/Flink 基本原理]] 由 GPT4 初步生成，后续根据学习内容及时调整。接下来一段时间就把时间和精力主要投入到大数据开发上面。根据网上的评价和招聘情况可以看出大数据方面非常缺人而开发岗则极度火热，究其原因可能是大数据相关的知识大而杂但是到岗位和业务之后又会变得细而精，学习起来不容易上手而且国内没有太多相关的中文文档，网上的知识也是寥寥可数，培训班对于这个方向也没有像前后端那样完整的培训体系。\n\n之后会把时间投入到数据学习中，[[大数据/转行数据理由|为什么呢]]？大数据开发在 AI 蓬勃发展之后也会有一大波进步的空间，现在国内在大数据开发上精通的人相对开发岗位的人来说少得多，而且中文文档和培训资料也是少得多，就身边人就业情况来说目前还没听说过大数据开发这个方向的，大部分都是做游戏或者普通开发岗。\n\n大数据开发要学习的内容和知识点相当多，基本计算机内容都会涵盖到包括操作系统、Linux 命令、容器启停、集群运维还有各种持久化层例如 ES、Redis、Hbase、ClickHouse 等等。仅拿前端来对比，要比前端熟悉和会的知识点多得多，实际上前端主要掌握 JS、CSS 两件套就够了，但数据开发要掌握的却是整个计算机知识体系。我喜欢做有挑战的事情，最近更是希望自己做能长期坚持下去的事情，而大数据开发则是一个很好的方向，**但是要同时切记不管什么知识能用到生活中、工作中为人类社会带来价值的事情才是有意义的知识，不要一味地钻研知识从而掉入了唯知识论的思维中，觉得万般皆下品，惟有读书高。不去做能为他人带来价值为社会带来价值的事情，长期下来只会觉得内心空虚**。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-19":{"title":"2023-04-19","content":"\n由于经常逛推而且收藏一些 thread，但是 twitter 看历史以及收藏的功能做的实在是太烂而且格式也不统一并且不支持发送长文所以基本上保存就等于学会，实际上基本不会再点开一次，很多优秀的文章也因此没有机会再完整阅读了，最近想利用浏览器插件在推特保存 thread 过后自动向 github仓库提交文件，然后后续本地仓库 pull 一下就行(最好设置一个自动化流程，每天早上自动同步)。该想法已经被否决(已经有了成熟的产品 readwise，基本集成了网页端全部的应用，都可以做笔记然后导出到笔记软件上)\n\n[[收藏句子]]\n\n","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-22":{"title":"2023-04-22","content":"\n安装了一个可以支持大部分自定义的主题：[[obsidian/Anuppuccin]]，通过 Style Setting 插件来自定义大部分样式包括主题色、MD 文件样式、Layout 样式等等，可以说有非常高的自定义能力了，如果后续有时间可以出一个教程来使用这款主题。\n","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-23":{"title":"2023-04-23","content":"\n昨天熬夜主要搞了 Obsidian 的美化操作，将主题和字体设置到了自己舒服的程度，之前一直使用默认的界面感觉并不是很舒服，而且很多 **markdown 展示** 都不完善例如大标题只有大小没有颜色的区别让人分不清楚，同理**加粗**和*斜体*也是这样。之前觉得 obsidian 没有插件也能很好的使用，但是就目前的情况来说少了一些辅助插件去写文章就会时不时的多出一些心智负担，比如说经常忘记一些 md 语法像是插入图片、插入待办等等，这个时候有一个辅助 md 提示就会方便很多。**所以有的时候插件并不是越少越好也不是越多越好，仅仅取决于供需关系**。如果你希望写作时的负担小一些而且使用插件并不会大于不使用的心智负担就尽可能的去使用插件来为自己所用。\n\n### 关于 obsidian 使用的插件\n1. [[Calendar]]\n2. Obsidian git\n3. [[Outliner]]\n4. [[Style Settings]]","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-24":{"title":"2023-04-24","content":"\n看完了[[漫画/亲爱的我包含杀意]]，相当优秀的悬疑智斗漫画，最后的主题也一并升华。\n\n最近漫画看的实在有点太多了，基本都是悬疑智斗类，搞得自己在晚上的时候也有点疑神疑鬼的。看这类漫画的意义一方面是满足自己推理和猜测的好奇心另一方面是观察人性之恶到底能有多么没有底线。\n\n今天又装了 [[tracker]] 和 [[cMenu]] 两款插件，感觉对于我的 obsidian 而言又多了便捷性和统计性。\n\n### 工作\n- [x] 数据 DDH 迁移，审批通过后 PMS 打包\n- [x] 文档迁移完成，检查无语病无展示问题后提交到群里一起过一下\n- [x] 数据地图一批次功能自测提测\n- [x] 有时间继续开发重构 Flink 作业代码\n\n### 生活\n- [x] 使用番茄钟来规范自己的专注时间\n- [x] 如交易一样，凡事都应该至少有一份粗略的计划，在什么时间段该干什么应该都是大概列好的，这样能得到什么的结果也是大概知道的，否则就如无头苍蝇般根本不知道明天未来会发生什么，进步也就因此停止\n","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%AF%8F%E6%97%A5/2023-04-25":{"title":"2023-04-25","content":"\u003e [!NOTE] Daily Quote\n\u003e \u003e Honesty is the best policy.\n\u003e — \u003ccite\u003eBenjamin Franklin\u003c/cite\u003e\n\u003e \n\u003e ![photo by Gracia Dharma on Unsplash](https://images.unsplash.com/photo-1607604276583-eef5d076aa5f?crop=entropy\u0026cs=srgb\u0026fm=jpg\u0026ixid=MnwzNjM5Nzd8MHwxfHJhbmRvbXx8fHx8fHx8fDE2ODIzNTY0Mjg\u0026ixlib=rb-4.0.3\u0026q=85)\n\n根据 [[Templater]] 插件重新生成了模板文档，并且给主要的模板文档添加了快捷键。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%BC%AB%E7%94%BB/%E4%BA%B2%E7%88%B1%E7%9A%84%E6%88%91%E5%8C%85%E5%90%AB%E6%9D%80%E6%84%8F":{"title":"亲爱的我包含杀意","content":"\n未完待续……","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%BC%AB%E7%94%BB/%E6%88%91%E5%BF%83%E9%87%8C%E5%8D%B1%E9%99%A9%E7%9A%84%E4%B8%9C%E8%A5%BF":{"title":"我心里危险的东西","content":"\n真正的熬夜看完了这本漫画，大抵算是人生中真正意义上的第一次熬夜(指超过早上八点不睡觉)","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E6%BC%AB%E7%94%BB/Anime-%E8%8A%B1%E5%9B%AD":{"title":"Anime 花园","content":"\n这里是 Anime 花园，请多指教！\n[[漫画/我心里危险的东西]]\n[[漫画/亲爱的我包含杀意]]","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/%E7%BD%91%E9%A1%B5%E4%B8%8A%E4%BC%A0%E5%8E%9F%E7%90%86":{"title":"网页上传原理","content":"**FileReader** 对象用于读取用户在浏览器中选择的文件。这在处理文件上传、读取文件内容以显示在页面上或解析文件数据时非常有用。\n\nFileReader 对象包含了一系列方法和事件，让你能够以不同的方式读取文件数据，例如：\n1.  readAsText(file, encoding): 以文本形式读取文件内容，可以指定编码方式，通常是 \"UTF-8\"。\n2.  readAsDataURL(file): 读取文件内容并将其转换为 base64 编码的 data URL，这在显示图片或其他媒体文件时非常有用。\n3.  readAsArrayBuffer(file): 以 ArrayBuffer 形式读取文件内容，这在处理二进制数据时非常有用，比如图像、音频或视频文件。\n\nFileReader 对象还包含了一些事件，例如：\n1.  onload: 当文件读取操作成功完成时触发。\n2.  onerror: 当文件读取操作失败时触发。\n3.  onprogress: 当文件读取操作正在进行时触发。\n\n### 示例代码\n``` html\n\u003cinput type=\"file\" id=\"inputImage\"\u003e\n\u003cimg id=\"displayImage\" alt=\"Selected image\"\u003e\n```\n\n\n``` javascript\nconst inputImage = document.getElementById('inputImage');\nconst displayImage = document.getElementById('displayImage');\n\ninputImage.addEventListener('change', (event) =\u003e {\n  const file = event.target.files[0]; // 获取用户选择的第一个文件\n  if (file \u0026\u0026 file.type.startsWith('image/')) { // 确保文件是图像\n    const reader = new FileReader();\n\n    // 当读取操作成功完成时，将图片的data URL设置为\u003cimg\u003e标签的src属性\n    reader.onload = (e) =\u003e {\n      displayImage.src = e.target.result;\n    };\n\n    // 使用readAsDataURL方法读取图像文件\n    reader.readAsDataURL(file);\n  } else {\n    alert('Please select a valid image file.');\n  }\n});\n\n```\n\n`reader.readAsDataURL(file)`是必不可少的。实际上，`e.target.result`中的base64编码的图片数据是在`reader.readAsDataURL(file)`执行后得到的。`readAsDataURL()`方法是`FileReader`对象用来读取文件内容并将其转换为base64编码的DataURL的函数。当这个方法被调用时，FileReader对象会开始读取指定的文件并在完成时触发`onload`事件。","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/Tracker":{"title":"Tracker","content":"```tracker\nsearchType: frontmatter\nsearchTarget: pomodoro\nfolder: 每日\nstartDate: -1w\nendDate: 0d\nfitPanelWidth: false\nfixedScale: 1.3\nline:\n\ttitle: 番茄钟\n\tyAxisLabel: 个数\n\tyAxisUnit: 25min/个\n\txAxisLabel: D\n\tlineColor: yellow\n```\n```tracker\nsearchType: frontmatter\nsearchTarget: pomodoro\nfolder: 每日\ndatasetName: 番茄钟\nsummary:\n\ttemplate: \"平均番茄钟个数：{{average()}}\"\n```\n\n``` tracker\nsearchType: tag\nsearchTarget: tagName\nfolder: /\nstartDate:\nendDate:\nbar:\n    title: \"Bar Chart\"\n    xAxisLabel: Date\n    yAxisLabel: Value\n```\n","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/TypeScript":{"title":"TypeScript","content":"\n待完善","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null},"/obsidian/Anuppuccin":{"title":"Anuppuccin","content":"\n\u003e [!NOTE] Anuppuccin\n\u003e 需要安装不少插件以及自定义\n\n## 插件","lastmodified":"2023-04-24T17:35:08.844047777Z","tags":null}}